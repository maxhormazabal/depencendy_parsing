{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxhormazabal/depencendy_parsing/blob/main/p2_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiIjtQIW-xwx"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "**Students:** Maximiliano Hormaz√°bal - Mutaz Abueisheh\n",
        "\n",
        "## Installation of dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgvUH3m47oHY",
        "outputId": "b73e09f1-fd7a-4401-de3b-3704c61d7aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqe_9RpC-94T"
      },
      "source": [
        "## Connecting to Google Drive to read the `.py` file that contains the functions to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owHKXmFn1BPs",
        "outputId": "a329cfc6-ae1c-4004-8383-0ee5db918169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Getting access to Google Drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIpSqrDi3HrT"
      },
      "outputs": [],
      "source": [
        "# Import our own functions (they are in a .py file on Google Drive)\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/MASTER\")\n",
        "from nlu_preprocessing_utils import *\n",
        "from conllu import parse\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SOj67OF_IYX"
      },
      "source": [
        "## Reading datasource from its origin on github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xYLYzM18CJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab95b44-db01-47a4-870b-7e2567862ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-train.conllu\n",
            "2380069/2380069 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-test.conllu\n",
            "187923/187923 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-dev.conllu\n",
            "152194/152194 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# English\n",
        "# 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu'\n",
        "# https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-train.conllu\n",
        "\n",
        "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/'\n",
        "file_basename = 'en_partut-ud'\n",
        "(en_train,en_test,en_val) = readConlluDataset(base_url,file_basename)\n",
        "en_upo2number, en_number2upo, en_nupos = getUposList(en_train)\n",
        "number2action,action2number = getActionDict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm nlu_data/original_test.conllu\n",
        "generateConlluForTesting()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtLX9WVJgJp3",
        "outputId": "dd826e00-9914-4fb2-d295-3fffba759cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original file generated in nlu_data/original_test_line.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNc5Q7KP_Mz2"
      },
      "source": [
        "## Transforming the data source into the initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4LZejqf8QKV"
      },
      "outputs": [],
      "source": [
        "train_df = conlluToDatasetForDependency(en_train,en_upo2number)\n",
        "test_df = conlluToDatasetForDependency(en_test,en_upo2number)\n",
        "val_df = conlluToDatasetForDependency(en_val,en_upo2number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iYQ0mso4wDW"
      },
      "source": [
        "### Checking Projective Arcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbVcuJGzvz3Q"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.iloc[projectiveArcs(train_df)]\n",
        "test_df = test_df.iloc[projectiveArcs(test_df)]\n",
        "val_df = val_df.iloc[projectiveArcs(val_df)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woj7d4j97H01"
      },
      "source": [
        "## Training the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB-ErugJ7HRu"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "text = \"root\"\n",
        "\n",
        "for sentence in train_df['form']:\n",
        "  for word in sentence:\n",
        "    text = text + \" \" + word\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\",filters=\"\") \n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "#save\n",
        "with open('nlu_data/tokenizer.json', 'w') as outfile:\n",
        "    outfile.write(tokenizer.to_json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EecN_ANSJTf"
      },
      "source": [
        "## Post-Oracle datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(2,3),(3,3),(5,5),(6,6),(8,8),(10,10),(12,12)"
      ],
      "metadata": {
        "id": "92AzPPlmZ6k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5tru6AGOArC"
      },
      "outputs": [],
      "source": [
        "stack_len = 12\n",
        "buffer_len = 12\n",
        "(x_train,action_train,deprel_train) = transformByOracle(train_df,stack_len,buffer_len,en_nupos)\n",
        "(x_test,action_test,deprel_test) = transformByOracle(test_df,stack_len,buffer_len,en_nupos)\n",
        "(x_val,action_val,deprel_val) = transformByOracle(val_df,stack_len,buffer_len,en_nupos)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deprel_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6BQd5pSkHGt",
        "outputId": "6ec3057b-aba2-4513-a5b5-7885da365edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['None', 'None', 'None', ..., 'punct', 'None', 'None'], dtype='<U21')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvxR1N9VSZGw"
      },
      "source": [
        "## X-Variables Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Miykzx2PBc"
      },
      "outputs": [],
      "source": [
        "x_train_token = applyTokenizer(x_train,stack_len,buffer_len,tokenizer)\n",
        "x_test_token = applyTokenizer(x_test,stack_len,buffer_len,tokenizer)\n",
        "x_val_token = applyTokenizer(x_val,stack_len,buffer_len,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnDgk2riSd04"
      },
      "source": [
        "## Y-Variables encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u1S3Q1aSisk"
      },
      "outputs": [],
      "source": [
        "number2deprel,deprel2number = getDeprelDict(deprel_train)\n",
        "\n",
        "deprel_train,number2deprel_train,deprel2number_train = deprelToNumericalByTrain(deprel_train,number2deprel,deprel2number)\n",
        "deprel_test,number2deprel_test,deprel2number_test = deprelToNumericalByTrain(deprel_test,number2deprel,deprel2number)\n",
        "deprel_val,number2deprel_val,deprel2number_val = deprelToNumericalByTrain(deprel_val,number2deprel,deprel2number)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_encod_train = tf.keras.utils.to_categorical(action_train)\n",
        "deprel_encod_train = tf.keras.utils.to_categorical(deprel_train)\n",
        "action_encod_test = tf.keras.utils.to_categorical(action_test)\n",
        "deprel_encod_test = tf.keras.utils.to_categorical(deprel_test)\n",
        "action_encod_val = tf.keras.utils.to_categorical(action_val)\n",
        "deprel_encod_val = tf.keras.utils.to_categorical(deprel_val)"
      ],
      "metadata": {
        "id": "lVZErnySihpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz78ero_mu9C"
      },
      "source": [
        "### Making lengths of deprel equal between sets. Using the maximum value of deprels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VnhqHyplkU-"
      },
      "outputs": [],
      "source": [
        "max_len = max([deprel_encod_train.shape[1],deprel_encod_test.shape[1],deprel_encod_val.shape[1]])\n",
        "\n",
        "deprel_encod_train = tf.keras.utils.pad_sequences(deprel_encod_train,maxlen=max_len,padding='post')\n",
        "deprel_encod_test = tf.keras.utils.pad_sequences(deprel_encod_test,maxlen=max_len,padding='post')\n",
        "deprel_encod_val = tf.keras.utils.pad_sequences(deprel_encod_val,maxlen=max_len,padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAAazY0lYlsk"
      },
      "source": [
        "Creating folder to save the data depending of the size of stack and buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8xi2j9DYlHA"
      },
      "outputs": [],
      "source": [
        "folder_name = str(stack_len)+\"stack\"+str(buffer_len)+\"buffer\"\n",
        "path = \"nlu_data/\"+folder_name\n",
        "!mkdir -p {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vygXFJpeNJ7C"
      },
      "outputs": [],
      "source": [
        "# function to save the numpy array as a file and re-use it avoiding preprocessing steps\n",
        "# It will be save in your current directory (seted in \"os.chdir(\"/content/drive/MyDrive/MASTER\")\")\n",
        "\n",
        "# Saving train data\n",
        "np.save(path+'/x_train.npy', x_train_token) \n",
        "np.save(path+'/action_train.npy', action_encod_train)\n",
        "np.save(path+'/deprel_train.npy', deprel_encod_train)\n",
        "\n",
        "# Saving test data\n",
        "np.save(path+'/x_test.npy', x_test_token) \n",
        "np.save(path+'/action_test.npy', action_encod_test)\n",
        "np.save(path+'/deprel_test.npy', deprel_encod_test)\n",
        "\n",
        "# Saving val data\n",
        "np.save(path+'/x_val.npy', x_val_token) \n",
        "np.save(path+'/action_val.npy', action_encod_val)\n",
        "np.save(path+'/deprel_val.npy', deprel_encod_val)\n",
        "\n",
        "# With the following you can read the file and create the numpy array again\n",
        "# new_x_data = np.load('x_data.npy',allow_pickle=True)\n",
        "# new_action_data = np.load('action_data.npy',allow_pickle=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKkNmLezKGRnPQZbXQtVWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
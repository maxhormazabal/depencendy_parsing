{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxhormazabal/depencendy_parsing/blob/main/p2_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiIjtQIW-xwx"
      },
      "source": [
        "# Preprocesamiento\n",
        "\n",
        "## Instalaci√≥n de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgvUH3m47oHY",
        "outputId": "97c47ad1-25e1-48b9-f9cf-0d62d1cf67ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.8/dist-packages (4.5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqe_9RpC-94T"
      },
      "source": [
        "## Conectando con Google Drive para leer el archivo `.py` que contiene las funciones a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owHKXmFn1BPs",
        "outputId": "da700cf9-2e63-48b3-b6d0-80eba65e07c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Getting access to Google Drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIpSqrDi3HrT"
      },
      "outputs": [],
      "source": [
        "# Import our own functions (they are in a .py file on Google Drive)\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/MASTER\")\n",
        "from nlu_preprocessing_utils import *\n",
        "from conllu import parse\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing in One Step"
      ],
      "metadata": {
        "id": "aXfZhJypTFoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/'\n",
        "file_basename = 'en_partut-ud'\n",
        "stack_len = 7\n",
        "buffer_len = 10\n",
        "\n",
        "(path,x_train_token,action_encod_train,deprel_encod_train,x_test_token,action_encod_test,deprel_encod_test,x_val_token,action_encod_val,deprel_encod_val) = preprocessingOneStep(base_url,file_basename,stack_len,buffer_len)\n",
        "!mkdir -p {path}\n",
        "saveData(path,x_train_token,action_encod_train,deprel_encod_train,x_test_token,action_encod_test,deprel_encod_test,x_val_token,action_encod_val,deprel_encod_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igfT-ibOTFBq",
        "outputId": "5632c647-cd90-4ca3-ff96-5235e05c8b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data sucessfully saved on ./ nlu_data/7stack10buffer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SOj67OF_IYX"
      },
      "source": [
        "## Leyendo fuente de datos desde su origen en github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xYLYzM18CJL",
        "outputId": "7b341ab8-3773-473b-98c7-3efc77414849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of different UPOS:  17\n",
            "{'NOUN': 1, 'ADP': 2, 'DET': 3, 'AUX': 4, 'PART': 5, 'VERB': 6, 'PUNCT': 7, 'PROPN': 8, 'CCONJ': 9, 'ADJ': 10, 'ADV': 11, 'PRON': 12, 'NUM': 13, 'SCONJ': 14, 'X': 15, 'INTJ': 16, 'SYM': 17}\n"
          ]
        }
      ],
      "source": [
        "# English\n",
        "# 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu'\n",
        "# https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-train.conllu\n",
        "\n",
        "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/'\n",
        "file_basename = 'en_partut-ud'\n",
        "(en_train,en_test,en_val) = readConlluDataset(base_url,file_basename)\n",
        "en_upo2number, en_number2upo, en_nupos = getUposList(en_train)\n",
        "number2action,action2number = getActionDict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNc5Q7KP_Mz2"
      },
      "source": [
        "## Transformando la fuente de datos en el dataset inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4LZejqf8QKV"
      },
      "outputs": [],
      "source": [
        "train_df = conlluToDatasetForDependency(en_train,en_upo2number)\n",
        "test_df = conlluToDatasetForDependency(en_test,en_upo2number)\n",
        "val_df = conlluToDatasetForDependency(en_val,en_upo2number)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Projective Arcs"
      ],
      "metadata": {
        "id": "9iYQ0mso4wDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.iloc[projectiveArcs(train_df)]\n",
        "test_df = test_df.iloc[projectiveArcs(test_df)]\n",
        "val_df = val_df.iloc[projectiveArcs(val_df)]"
      ],
      "metadata": {
        "id": "JbVcuJGzvz3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenando Tokenizer"
      ],
      "metadata": {
        "id": "woj7d4j97H01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "text = \"root\"\n",
        "\n",
        "for sentence in train_df['form']:\n",
        "  for word in sentence:\n",
        "    text = text + \" \" + word\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\",filters=\"\") \n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "QB-ErugJ7HRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Oracle datasets"
      ],
      "metadata": {
        "id": "8EecN_ANSJTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack_len = 3\n",
        "buffer_len = 5\n",
        "(x_train,action_train,deprel_train) = transformByOracle(train_df,stack_len,buffer_len,en_nupos)\n",
        "(x_test,action_test,deprel_test) = transformByOracle(test_df,stack_len,buffer_len,en_nupos)\n",
        "(x_val,action_val,deprel_val) = transformByOracle(val_df,stack_len,buffer_len,en_nupos)"
      ],
      "metadata": {
        "id": "X5tru6AGOArC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## X-Variables Tokenization"
      ],
      "metadata": {
        "id": "VvxR1N9VSZGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_token = applyTokenizer(x_train,stack_len,buffer_len,tokenizer)\n",
        "x_test_token = applyTokenizer(x_test,stack_len,buffer_len,tokenizer)\n",
        "x_val_token = applyTokenizer(x_val,stack_len,buffer_len,tokenizer)"
      ],
      "metadata": {
        "id": "z8Miykzx2PBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y-Variables encoding"
      ],
      "metadata": {
        "id": "CnDgk2riSd04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deprel_train,number2deprel_train,deprel2number_train = deprelToNumerical(deprel_train)\n",
        "deprel_test,number2deprel_test,deprel2number_test = deprelToNumerical(deprel_test)\n",
        "deprel_val,number2deprel_val,deprel2number_val = deprelToNumerical(deprel_val)\n",
        "\n",
        "action_encod_train = tf.keras.utils.to_categorical(action_train)\n",
        "deprel_encod_train = tf.keras.utils.to_categorical(deprel_train)\n",
        "action_encod_test = tf.keras.utils.to_categorical(action_test)\n",
        "deprel_encod_test = tf.keras.utils.to_categorical(deprel_test)\n",
        "action_encod_val = tf.keras.utils.to_categorical(action_val)\n",
        "deprel_encod_val = tf.keras.utils.to_categorical(deprel_val)"
      ],
      "metadata": {
        "id": "9u1S3Q1aSisk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making lengths of deprel equal between sets. Using the maximum value of deprels"
      ],
      "metadata": {
        "id": "iz78ero_mu9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([deprel_encod_train.shape[1],deprel_encod_test.shape[1],deprel_encod_val.shape[1]])\n",
        "\n",
        "deprel_encod_train = tf.keras.utils.pad_sequences(deprel_encod_train,maxlen=max_len,padding='post')\n",
        "deprel_encod_test = tf.keras.utils.pad_sequences(deprel_encod_test,maxlen=max_len,padding='post')\n",
        "deprel_encod_val = tf.keras.utils.pad_sequences(deprel_encod_val,maxlen=max_len,padding='post')"
      ],
      "metadata": {
        "id": "-VnhqHyplkU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(deprel_encod_train.shape,deprel_encod_test.shape,deprel_encod_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq0B1Hyil35-",
        "outputId": "484f5a59-14f7-4b3b-bd37-496c836e10b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(86430, 41) (6854, 41) (5434, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating folder to save the data depending of the size of stack and buffer"
      ],
      "metadata": {
        "id": "AAAazY0lYlsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = str(stack_len)+\"stack\"+str(buffer_len)+\"buffer\"\n",
        "path = \"nlu_data/\"+folder_name\n",
        "!mkdir -p {path}"
      ],
      "metadata": {
        "id": "R8xi2j9DYlHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to save the numpy array as a file and re-use it avoiding preprocessing steps\n",
        "# It will be save in your current directory (seted in \"os.chdir(\"/content/drive/MyDrive/MASTER\")\")\n",
        "\n",
        "# Saving train data\n",
        "np.save(path+'/x_train.npy', x_train_token) \n",
        "np.save(path+'/action_train.npy', action_encod_train)\n",
        "np.save(path+'/deprel_train.npy', deprel_encod_train)\n",
        "\n",
        "# Saving test data\n",
        "np.save(path+'/x_test.npy', x_test_token) \n",
        "np.save(path+'/action_test.npy', action_encod_test)\n",
        "np.save(path+'/deprel_test.npy', deprel_encod_test)\n",
        "\n",
        "# Saving val data\n",
        "np.save(path+'/x_val.npy', x_val_token) \n",
        "np.save(path+'/action_val.npy', action_encod_val)\n",
        "np.save(path+'/deprel_val.npy', deprel_encod_val)\n",
        "\n",
        "# With the following you can read the file and create the numpy array again\n",
        "# new_x_data = np.load('x_data.npy',allow_pickle=True)\n",
        "# new_action_data = np.load('action_data.npy',allow_pickle=True)"
      ],
      "metadata": {
        "id": "vygXFJpeNJ7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbfhmS03ZAdgfSbvTtvJaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxhormazabal/depencendy_parsing/blob/main/P2_Preprocessing_NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiIjtQIW-xwx"
      },
      "source": [
        "# Preprocesamiento\n",
        "\n",
        "## Instalaci√≥n de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgvUH3m47oHY",
        "outputId": "61dd9cb8-ad60-42e0-8607-18019cbf2f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqe_9RpC-94T"
      },
      "source": [
        "## Conectando con Google Drive para leer el archivo `.py` que contiene las funciones a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owHKXmFn1BPs",
        "outputId": "97370438-3af8-481f-d451-8da3273aa5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Getting access to Google Drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIpSqrDi3HrT"
      },
      "outputs": [],
      "source": [
        "# Import our own functions (they are in a .py file on Google Drive)\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/MASTER\")\n",
        "from nlu_preprocessing_utils import *\n",
        "from conllu import parse\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SOj67OF_IYX"
      },
      "source": [
        "## Leyendo fuente de datos desde su origen en github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xYLYzM18CJL",
        "outputId": "5d6beab0-a477-4acb-f160-e91f257e6e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-train.conllu\n",
            "2380069/2380069 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-test.conllu\n",
            "187923/187923 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-dev.conllu\n",
            "152194/152194 [==============================] - 0s 0us/step\n",
            "Total of different UPOS:  17\n",
            "{'NOUN': 1, 'ADP': 2, 'DET': 3, 'AUX': 4, 'PART': 5, 'VERB': 6, 'PUNCT': 7, 'PROPN': 8, 'CCONJ': 9, 'ADJ': 10, 'ADV': 11, 'PRON': 12, 'NUM': 13, 'SCONJ': 14, 'X': 15, 'INTJ': 16, 'SYM': 17}\n"
          ]
        }
      ],
      "source": [
        "# English\n",
        "# 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu'\n",
        "# https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/en_partut-ud-train.conllu\n",
        "\n",
        "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/'\n",
        "file_basename = 'en_partut-ud'\n",
        "(en_train,en_test,en_val) = readConlluDataset(base_url,file_basename)\n",
        "en_upo2number, en_number2upo, en_nupos = getUposList(en_train)\n",
        "number2action,action2number = getActionDict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNc5Q7KP_Mz2"
      },
      "source": [
        "## Transformando la fuente de datos en el dataset inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4LZejqf8QKV"
      },
      "outputs": [],
      "source": [
        "train_df = conlluToDatasetForDependency(en_train,en_upo2number)\n",
        "test_df = conlluToDatasetForDependency(en_test,en_upo2number)\n",
        "val_df = conlluToDatasetForDependency(en_val,en_upo2number)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenando Tokenizer"
      ],
      "metadata": {
        "id": "woj7d4j97H01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "text = \"root\"\n",
        "\n",
        "for sentence in train_df['form']:\n",
        "  for word in sentence:\n",
        "    text = text + \" \" + word\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\",filters=\"\") \n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "QB-ErugJ7HRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Oracle datasets"
      ],
      "metadata": {
        "id": "8EecN_ANSJTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack_len = 10\n",
        "buffer_len = 15"
      ],
      "metadata": {
        "id": "ZTu7iD6iWV2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EKWlISulsSx",
        "outputId": "fe1a3fea-f56c-427d-df7e-0858168a41cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 3, ..., 4, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deprel_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDtvhibel0Hk",
        "outputId": "3df9c7b6-c236-4611-d06a-33a69b710485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([32., 31.,  0., ...,  0.,  0.,  0.])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,action_train,deprel_train) = transformByOracle(train_df,stack_len,buffer_len,en_nupos)\n",
        "(x_test,action_test,deprel_test) = transformByOracle(test_df,stack_len,buffer_len,en_nupos)\n",
        "(x_val,action_val,deprel_val) = transformByOracle(val_df,stack_len,buffer_len,en_nupos)"
      ],
      "metadata": {
        "id": "LRP4ZTcvC_VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_clean = np.copy(x_train)\n",
        "x_test_clean = np.copy(x_test)\n",
        "x_val_clean = np.copy(x_val)\n",
        "\n",
        "action_train_clean = np.copy(action_train)\n",
        "deprel_train_clean = np.copy(deprel_train)\n",
        "action_test_clean = np.copy(action_test)\n",
        "deprel_test_clean = np.copy(deprel_test)\n",
        "action_val_clean = np.copy(action_val)\n",
        "deprel_val_clean = np.copy(deprel_val)"
      ],
      "metadata": {
        "id": "PGiu26B4thd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## X-Variables Tokenization"
      ],
      "metadata": {
        "id": "VvxR1N9VSZGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_token = applyTokenizer(x_train,stack_len,buffer_len,tokenizer)\n",
        "x_test_token = applyTokenizer(x_test,stack_len,buffer_len,tokenizer)\n",
        "x_val_token = applyTokenizer(x_val,stack_len,buffer_len,tokenizer)"
      ],
      "metadata": {
        "id": "z8Miykzx2PBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y-Variables encoding"
      ],
      "metadata": {
        "id": "CnDgk2riSd04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number2action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOWiXE0qjbBt",
        "outputId": "1380a17a-3206-438a-818e-57f891aa783b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'Right Arc', 2: 'Left Arc', 3: 'Shift', 4: 'Reduce', 5: 'Done'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deprel_train,number2deprel_train,deprel2number_train = deprelToNumerical(deprel_train)\n",
        "deprel_test,number2deprel_test,deprel2number_test = deprelToNumerical(deprel_test)\n",
        "deprel_val,number2deprel_val,deprel2number_val = deprelToNumerical(deprel_val)\n",
        "\n",
        "action_encod_train = tf.keras.utils.to_categorical(action_train)\n",
        "deprel_encod_train = tf.keras.utils.to_categorical(deprel_train)\n",
        "action_encod_test = tf.keras.utils.to_categorical(action_test)\n",
        "deprel_encod_test = tf.keras.utils.to_categorical(deprel_test)\n",
        "action_encod_val = tf.keras.utils.to_categorical(action_val)\n",
        "deprel_encod_val = tf.keras.utils.to_categorical(deprel_val)"
      ],
      "metadata": {
        "id": "9u1S3Q1aSisk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating folder to save the data depending of the size of stack and buffer"
      ],
      "metadata": {
        "id": "AAAazY0lYlsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = str(stack_len)+\"stack\"+str(buffer_len)+\"buffer\"\n",
        "path = \"nlu_data/\"+folder_name\n",
        "!mkdir -p {path}"
      ],
      "metadata": {
        "id": "R8xi2j9DYlHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to save the numpy array as a file and re-use it avoiding preprocessing steps\n",
        "# It will be save in your current directory (seted in \"os.chdir(\"/content/drive/MyDrive/MASTER\")\")\n",
        "\n",
        "# Saving train data\n",
        "np.save(path+'/x_train.npy', x_train_token) \n",
        "np.save(path+'/action_train.npy', action_encod_train)\n",
        "np.save(path+'/deprel_train.npy', deprel_encod_train)\n",
        "\n",
        "# Saving test data\n",
        "np.save(path+'/x_test.npy', x_test_token) \n",
        "np.save(path+'/action_test.npy', action_encod_test)\n",
        "np.save(path+'/deprel_test.npy', deprel_encod_test)\n",
        "\n",
        "# Saving val data\n",
        "np.save(path+'/x_val.npy', x_val_token) \n",
        "np.save(path+'/action_val.npy', action_encod_val)\n",
        "np.save(path+'/deprel_val.npy', deprel_encod_val)"
      ],
      "metadata": {
        "id": "vygXFJpeNJ7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With the following you can read the file and create the numpy array again\n",
        "new_x_data = np.load('x_data.npy',allow_pickle=True)\n",
        "new_action_data = np.load('action_data.npy',allow_pickle=True)"
      ],
      "metadata": {
        "id": "R4KepLH4QTQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM3zJci5tLfkDeNXLfmD+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
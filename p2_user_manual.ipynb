{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66117ff6",
   "metadata": {},
   "source": [
    "# P2 - User manual\n",
    "\n",
    "**Students:** Maximiliano Hormaz√°bal - Mutaz Abueisheh\n",
    "\n",
    "This project is available on GitHub. [Click here to go to the repository](https://github.com/maxhormazabal/depencendy_parsing)\n",
    "\n",
    "### 0. Identify the files:\n",
    "\n",
    "This project have two main files:\n",
    "- `p2_preprocessing.ipynb`: Is the one you have to use to know all the steps for get the raw data, make the transformations and load the final data set to use it after.\n",
    "- `p2_ann_models.ipynb`: Here you have all the model's arquitectures, in this program we do not transform the data it is just for experiments and get results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7f1c33",
   "metadata": {},
   "source": [
    "### 1. Install/Import libraries\n",
    "\n",
    "In this project files and folders are going to be created in some steps. If you are going to work on Google Colab it is crucial to connect your notebook to your Google Drive profile for files/folders managment doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting access to Google Drive files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7f1c33",
   "metadata": {},
   "source": [
    "\n",
    "The first step is to import the libraries we are going to use (or install if is necessary), the most important are:\n",
    "- Conllu to read the languages files correctly (in this case we will use parse module)\n",
    "- Tensorflow to work with ANN, Tokenizer and other Machine Learning Tools\n",
    "- Pandas to create/transform dataframes\n",
    "- Numpy to work with numbers and some data structures\n",
    "\n",
    "To install you can use `!pip install <name>` to install the specific tool that you need.\n",
    "\n",
    "As following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Setting working directory and importing functions\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/MASTER\") # <- Folder where you saved the utils\n",
    "from nlu_model_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85f7154a",
   "metadata": {},
   "source": [
    "## Preprocessing Notebook\n",
    "\n",
    "### 2. Download the langagues datasets\n",
    "\n",
    "To find the dataset that you want to use visit this <a href=\"https://github.com/UniversalDependencies/\" target=\"_blank\">Github Repository</a>, there are a many posibilities of languague. If you are looking for an specific language the \"Repositories\" search bar is useful to it. \n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/1.png\" width=\"700\">\n",
    "\n",
    "When you are ready do the following:\n",
    "- Go to the repository of your selected language\n",
    "- As you see you have 3 files: training, testing, validation (train, test, dev)\n",
    "\n",
    "For instance\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/2.png\" width=\"700\">\n",
    "\n",
    "- Click on one of them (could be anyone) and then click on \"View raw\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/3.png\" width=\"700\">\n",
    "\n",
    "- Here there is the raw file, this is the information that we need, our data. Just copy the URL.\n",
    "\n",
    "<div style=\"border: 1px solid #2596be;padding:10px;\">\n",
    "    <p style=\"text-align: center;\">**Important:** You do not need to repeat this process for thi others two files. We need only one URL for **each language**.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/4.png\" width=\"700\">\n",
    "\n",
    "### 3. Read the data with Python\n",
    "\n",
    "Now we have the address of the data, let's read this file and save as a Python data structure. This is an example with _English Data_ using the URL `https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu` ins this case is the URL for training but as we wrote previously, does not matter wich data set you chose. We just need one of them.\n",
    "\n",
    "Distribute the URL in two variables:\n",
    "\n",
    "    - base_url: Contain the URL until \"/master/\"\n",
    "    - file_basename: Contain the name of the language without the last score\n",
    "    \n",
    "It will be clear with the next example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680500a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "# 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu'\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/'\n",
    "file_basename = 'en_ewt-ud'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2b0e3d9",
   "metadata": {},
   "source": [
    "After you did this with all the languages that you want we can start with the preprocessing.\n",
    "\n",
    "<div style=\"border: 1px solid #2596be;padding:10px;\">\n",
    "    <p style=\"text-align: center;\"> <strong> Important:</strong> We are going to use self-made functions to make this process more understandable, the finall cell contains all the functions to be runned together (We must have this functions in memory to make the notebook works.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7533999",
   "metadata": {},
   "source": [
    "## 4. Preprocessing data\n",
    "\n",
    "The function `preprocessingOneStep` contains all the preprocessing steps just in one function. The output are the data sets and the path of that data. Actually the notebook of preproceesing is only in charge of this part of the project and the data we will use in models will be saved as files and the mentioned relative path.\n",
    "\n",
    "With the variables:\n",
    "\n",
    "```\n",
    "stack_len = 7\n",
    "buffer_len = 10\n",
    "```\n",
    "\n",
    "You can set the size of stack and buffer for the sets. Run this cell everytime you need to create a new data source for models\n",
    "\n",
    "The code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/'\n",
    "file_basename = 'en_partut-ud'\n",
    "stack_len = 7\n",
    "buffer_len = 10\n",
    "\n",
    "(path,\n",
    "x_train_token,action_encod_train,deprel_encod_train,\n",
    "x_test_token,action_encod_test,deprel_encod_test,\n",
    "x_val_token,action_encod_val,deprel_encod_val) = preprocessingOneStep(base_url,file_basename,stack_len,buffer_len)\n",
    "\n",
    "\n",
    "!mkdir -p {path}\n",
    "\n",
    "\n",
    "saveData(path,x_train_token,action_encod_train,deprel_encod_train,x_test_token,action_encod_test,deprel_encod_test,x_val_token,action_encod_val,deprel_encod_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "014acd0e",
   "metadata": {},
   "source": [
    "Notice that after the execution of `preprocessingOneStep` the line `!mkdir -p {path}` is runned because the `!` symbol let us work with the terminal prompt and create a directory to save the numpy files for each set.\n",
    "\n",
    "After creating the folder we use `saveData` for save the files in your Google Drive directory. This is a fundamental step because the model notebook is going to take the data just reading the files; that way we do not have to repeat the preprocessing everytime.\n",
    "\n",
    "**If you run many times the folder should be like this**\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal/depencendy_parsing/main/img/nlu_data_folder.png\" width=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e72f7a",
   "metadata": {},
   "source": [
    "## Models Notebook\n",
    "\n",
    "After generating the data needed to train the models, the neural network tests begin. For this it is necessary to know the relevant functions:\n",
    "\n",
    "1. `buildModelA` allows us to build and compile the model selecting all the important parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelA(stack_len,buffer_len,action_shape,deprel_shape,optimizer='adam',learning_rate=0.01,embedd_output=50,loss='categorical_crossentropy'):\n",
    "  input1 = tf.keras.layers.Input(shape=(stack_len,),name = 'Stack_Input')\n",
    "  input2 = tf.keras.layers.Input(shape=(buffer_len,),name = 'Buffer_Input')\n",
    "\n",
    "  embedding_layer = tf.keras.layers.Embedding(10000, embedd_output,mask_zero=True)\n",
    "  input1_embedded = embedding_layer(input1)\n",
    "  input2_embedded = embedding_layer(input2)\n",
    "\n",
    "  lstm1 = tf.keras.layers.LSTM(embedd_output,return_sequences=False,name=\"LSTM_Layer1\")(input1_embedded)\n",
    "  lstm2 = tf.keras.layers.LSTM(embedd_output,return_sequences=False,name=\"LSTM_Layer2\")(input2_embedded)\n",
    "\n",
    "  # Concatenamos a lo largo del √∫ltimo eje\n",
    "  merged = tf.keras.layers.Concatenate(axis=1,name = 'Concat_Layer')([lstm1, lstm2])\n",
    "  dense1 = tf.keras.layers.Dense(50, activation='sigmoid', use_bias=True,name = 'Dense_Layer1')(merged)\n",
    "  dense2 = tf.keras.layers.Dense(15, input_dim=1, activation='relu', use_bias=True,name = 'Dense_Layer2')(dense1)\n",
    "  dense3 = tf.keras.layers.Dense(30, input_dim=1, activation='relu', use_bias=True,name = 'Dense_Layer3')(dense2)\n",
    "  output1 = tf.keras.layers.Dense(action_shape, activation='softmax', use_bias=True,name = 'Action_Output')(dense3)\n",
    "  output2 = tf.keras.layers.Dense(deprel_shape, activation='softmax', use_bias=True,name = 'Deprel_Output')(dense3)\n",
    "\n",
    "  model = tf.keras.Model(inputs=[input1,input2],outputs=[output1,output2])\n",
    "  \n",
    "  if(optimizer.lower() == 'adam'):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  elif(optimizer.lower() == 'sgd'):\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "  elif(optimizer.lower() == 'rmsprop'):\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adamw'):\n",
    "    opt = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adadelta'):\n",
    "    opt = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adagrad'):\n",
    "    opt = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adamax'):\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adafactor'):\n",
    "    opt = tf.keras.optimizers.Adafactor(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'nadam'):\n",
    "    opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'ftrl'):\n",
    "    opt = tf.keras.optimizers.Ftrl(learning_rate=learning_rate)\n",
    "  else:\n",
    "    print('Optimizer not properly defined')\n",
    "\n",
    "  model.compile(loss=loss,optimizer=opt,metrics=['accuracy'])\n",
    "  return(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fba503c1",
   "metadata": {},
   "source": [
    "2. `fitModel` use the model created with the previous function and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "              x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "              x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "              model,\n",
    "              stopper,patience=3,epochs=10,batch_size=128):\n",
    "  callback = tf.keras.callbacks.EarlyStopping(monitor=stopper, patience=patience,restore_best_weights=True)\n",
    "  model.fit([x_train_stack,x_train_buffer],\n",
    "            [action_train, deprel_train],\n",
    "            epochs=epochs, batch_size=batch_size,\n",
    "            callbacks=[callback],\n",
    "            verbose = 0,\n",
    "            validation_data=([x_val_stack,x_val_buffer],[action_val,deprel_val]))\n",
    "  score = model.evaluate([x_test_stack,x_test_buffer],[action_test, deprel_test], verbose=0)\n",
    "  return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6837ab48",
   "metadata": {},
   "source": [
    "3. Finally `saveModelAData` is a function that can be used to test different configuration of an specific parameter. It trains a model changing the selected pivot parameter and save the results as a dataframe. The output of this function is the summarized data set and the best model of each execution.\n",
    "\n",
    "**Important**: The metrics are calcutated with the evaluation function (Keras) using the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6106bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModelAData(pivot_name,pivot,\n",
    "                   x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "                   x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "                   x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "                   stack_len,buffer_len,\n",
    "                   stopper,patience,\n",
    "                   batch_size,epochs,\n",
    "                   optimizer,learning_rate,\n",
    "                   embedd_output=50):\n",
    "  # Creating empty lists\n",
    "  arquitecture_set = []\n",
    "  stack_set = []\n",
    "  buffer_set = []\n",
    "  action_accuracy_set = []\n",
    "  deprel_accuracy_set = []\n",
    "  action_loss_set = []\n",
    "  deprel_loss_set = []\n",
    "  batch_size_set = []\n",
    "  epochs_set = []\n",
    "  optimizer_set = []\n",
    "  learning_rate_set = []\n",
    "  embedd_output_set = []\n",
    "  early_stop_set = []\n",
    "  time_set = []\n",
    "\n",
    "  models = []\n",
    "\n",
    "\n",
    "  for (index,value) in enumerate(pivot):\n",
    "\n",
    "    print('Starting execution where ',pivot_name,' varies, now with the value(s) ',value,'.')\n",
    "\n",
    "    if (pivot_name == 'batch_size'):\n",
    "      batch_size = value\n",
    "    elif (pivot_name == 'epochs'):\n",
    "      epochs = value\n",
    "    elif (pivot_name == 'early_stop'):\n",
    "      (stopper,patience) = value\n",
    "    elif (pivot_name == 'optimizer'):\n",
    "      (optimizer,learning_rate) = value\n",
    "\n",
    "    arquitecture = 'A'\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = buildModelA(stack_len,buffer_len,action_train[0].shape[0],deprel_train[0].shape[0],optimizer=optimizer,learning_rate=learning_rate,embedd_output=embedd_output,loss='categorical_crossentropy')\n",
    "\n",
    "    score = fitModel(x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "                  x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "                  x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "                  model,\n",
    "                  stopper,patience,epochs,batch_size)\n",
    "    loss,Action_Output_loss,Deprel_Output_loss,Action_Output_accuracy,Deprel_Output_accuracy = score\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "    # Saving models\n",
    "    models.append(model)\n",
    "\n",
    "    # Append values\n",
    "    arquitecture_set.append(arquitecture)\n",
    "    stack_set.append(stack_len)\n",
    "    buffer_set.append(buffer_len)\n",
    "    action_accuracy_set.append(Action_Output_accuracy)\n",
    "    deprel_accuracy_set.append(Deprel_Output_accuracy)\n",
    "    action_loss_set.append(Action_Output_loss)\n",
    "    deprel_loss_set.append(Deprel_Output_loss)\n",
    "    batch_size_set.append(batch_size)\n",
    "    epochs_set.append(epochs)\n",
    "    optimizer_set.append(optimizer)\n",
    "    learning_rate_set.append(learning_rate)\n",
    "    embedd_output_set.append(embedd_output)\n",
    "    early_stop_set.append(stopper)\n",
    "    time_set.append(training_time)\n",
    "\n",
    "  # Data dictionary\n",
    "\n",
    "  resultDict = {\n",
    "    'arquitecture' : arquitecture_set,\n",
    "    'stack' : stack_set,\n",
    "    'buffer' : buffer_set,\n",
    "    'action_accuracy' : action_accuracy_set,\n",
    "    'deprel_accuracy' : deprel_accuracy_set,\n",
    "    'action_loss' : action_loss_set,\n",
    "    'deprel_loss' : deprel_loss_set,\n",
    "    'batch_size' : batch_size_set,\n",
    "    'epochs' : epochs_set,\n",
    "    'optimizer' : optimizer_set,\n",
    "    'learning_rate' : learning_rate_set,\n",
    "    'embedd_output' : embedd_output,\n",
    "    'early_stop_set' : early_stop_set,\n",
    "    'time' : time_set,\n",
    "  }\n",
    "\n",
    "  return (pd.DataFrame(resultDict),models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "962d2f8f",
   "metadata": {},
   "source": [
    "One execution of those three function together is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different optimizers\n",
    "\n",
    "pivot_name = 'optimizer'\n",
    "pivot = [\n",
    "    ('adam' , 0.001),\n",
    "    ('adam' , 0.01),\n",
    "    ('adam' , 0.1),\n",
    "    ('sgd' , 0.001),\n",
    "    ('sgd' , 0.01),\n",
    "    ('sgd' , 0.1),\n",
    "    ('rmsprop' , 0.001),\n",
    "    ('rmsprop' , 0.01),\n",
    "    ('rmsprop' , 0.1),\n",
    "    ('adadelta' , 0.001),\n",
    "    ('adadelta' , 0.01),\n",
    "    ('adadelta' , 0.1),\n",
    "    ('adagrad' , 0.001),\n",
    "    ('adagrad' , 0.01),\n",
    "    ('adagrad' , 0.1),\n",
    "    ('adamax' , 0.001),\n",
    "    ('adamax' , 0.01),\n",
    "    ('adamax' , 0.1),\n",
    "    ('nadam' , 0.001),\n",
    "    ('nadam' , 0.01),\n",
    "    ('nadam' , 0.1),\n",
    "    ('ftrl' , 0.001),\n",
    "    ('ftrl' , 0.01),\n",
    "    ('ftrl' , 0.1)\n",
    "]\n",
    "\n",
    "# Setting values\n",
    "stack_len = 3\n",
    "buffer_len = 3\n",
    "arquitecture = 'A'\n",
    "stopper = 'val_Deprel_Output_loss'\n",
    "patience = 3\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "optimizer = 'adam'\n",
    "learning_rate = 0.01\n",
    "embedd_output = 50\n",
    "\n",
    "(df_res_optimizers,models_optimizers) = saveModelAData(pivot_name,pivot,\n",
    "                   x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "                   x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "                   x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "                   stack_len,buffer_len,\n",
    "                   stopper,patience,\n",
    "                   batch_size,epochs,\n",
    "                   optimizer,learning_rate,\n",
    "                   embedd_output=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de860f7a",
   "metadata": {},
   "source": [
    "Where the data frame of result is like this\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal/depencendy_parsing/main/img/df_results_example.png\" width=\"700\">\n",
    "\n",
    "And you can find all the data set with the results (based on a pivot parameter) in the folder `model_testing` in `.csv` format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

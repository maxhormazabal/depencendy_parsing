{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66117ff6",
   "metadata": {},
   "source": [
    "# P2 - User manual\n",
    "\n",
    "**Students:** Maximiliano Hormaz√°bal - Mutaz Abueisheh\n",
    "\n",
    "This project is available on GitHub. [Click here to go to the repository](https://github.com/maxhormazabal/depencendy_parsing)\n",
    "\n",
    "### 0. Identify the files:\n",
    "\n",
    "This project have two main files:\n",
    "- `p2_preprocessing.ipynb`: Is the one you have to use to know all the steps for get the raw data, make the transformations and load the final data set to use it after.\n",
    "- `p2_ann_models.ipynb`: Here you have all the model's arquitectures, in this program we do not transform the data it is just for experiments and get results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7f1c33",
   "metadata": {},
   "source": [
    "### 1. Install/Import libraries\n",
    "\n",
    "In this project files and folders are going to be created in some steps. If you are going to work on Google Colab it is crucial to connect your notebook to your Google Drive profile for files/folders managment doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting access to Google Drive files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7f1c33",
   "metadata": {},
   "source": [
    "\n",
    "The first step is to import the libraries we are going to use (or install if is necessary), the most important are:\n",
    "- Conllu to read the languages files correctly (in this case we will use parse module)\n",
    "- Tensorflow to work with ANN, Tokenizer and other Machine Learning Tools\n",
    "- Pandas to create/transform dataframes\n",
    "- Numpy to work with numbers and some data structures\n",
    "\n",
    "To install you can use `!pip install <name>` to install the specific tool that you need.\n",
    "\n",
    "As following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Setting working directory and importing functions\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/MASTER\") # <- Folder where you saved the utils\n",
    "from nlu_model_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85f7154a",
   "metadata": {},
   "source": [
    "## Preprocessing Notebook\n",
    "\n",
    "### 2. Download the langagues datasets\n",
    "\n",
    "To find the dataset that you want to use visit this <a href=\"https://github.com/UniversalDependencies/\" target=\"_blank\">Github Repository</a>, there are a many posibilities of languague. If you are looking for an specific language the \"Repositories\" search bar is useful to it. \n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/1.png\" width=\"700\">\n",
    "\n",
    "When you are ready do the following:\n",
    "- Go to the repository of your selected language\n",
    "- As you see you have 3 files: training, testing, validation (train, test, dev)\n",
    "\n",
    "For instance\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/2.png\" width=\"700\">\n",
    "\n",
    "- Click on one of them (could be anyone) and then click on \"View raw\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/3.png\" width=\"700\">\n",
    "\n",
    "- Here there is the raw file, this is the information that we need, our data. Just copy the URL.\n",
    "\n",
    "<div style=\"border: 1px solid #2596be;padding:10px;\">\n",
    "    <p style=\"text-align: center;\">**Important:** You do not need to repeat this process for thi others two files. We need only one URL for **each language**.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/4.png\" width=\"700\">\n",
    "\n",
    "### 3. Read the data with Python\n",
    "\n",
    "Now we have the address of the data, let's read this file and save as a Python data structure. This is an example with _English Data_ using the URL `https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu` ins this case is the URL for training but as we wrote previously, does not matter wich data set you chose. We just need one of them.\n",
    "\n",
    "Distribute the URL in two variables:\n",
    "\n",
    "    - base_url: Contain the URL until \"/master/\"\n",
    "    - file_basename: Contain the name of the language without the last score\n",
    "    \n",
    "It will be clear with the next example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680500a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "# 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu'\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/'\n",
    "file_basename = 'en_ewt-ud'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2b0e3d9",
   "metadata": {},
   "source": [
    "After you did this with all the languages that you want we can start with the preprocessing.\n",
    "\n",
    "<div style=\"border: 1px solid #2596be;padding:10px;\">\n",
    "    <p style=\"text-align: center;\"> <strong> Important:</strong> We are going to use self-made functions to make this process more understandable, the finall cell contains all the functions to be runned together (We must have this functions in memory to make the notebook works.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7533999",
   "metadata": {},
   "source": [
    "## 4. Preprocessing data\n",
    "\n",
    "The function `preprocessingOneStep` contains all the preprocessing steps just in one function. The output are the data sets and the path of that data. Actually the notebook of preproceesing is only in charge of this part of the project and the data we will use in models will be saved as files and the mentioned relative path.\n",
    "\n",
    "With the variables:\n",
    "\n",
    "```\n",
    "stack_len = 7\n",
    "buffer_len = 10\n",
    "```\n",
    "\n",
    "You can set the size of stack and buffer for the sets. Run this cell everytime you need to create a new data source for models\n",
    "\n",
    "The code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/'\n",
    "file_basename = 'en_partut-ud'\n",
    "stack_len = 7\n",
    "buffer_len = 10\n",
    "\n",
    "(path,\n",
    "x_train_token,action_encod_train,deprel_encod_train,\n",
    "x_test_token,action_encod_test,deprel_encod_test,\n",
    "x_val_token,action_encod_val,deprel_encod_val) = preprocessingOneStep(base_url,file_basename,stack_len,buffer_len)\n",
    "\n",
    "\n",
    "!mkdir -p {path}\n",
    "\n",
    "\n",
    "saveData(path,x_train_token,action_encod_train,deprel_encod_train,x_test_token,action_encod_test,deprel_encod_test,x_val_token,action_encod_val,deprel_encod_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "014acd0e",
   "metadata": {},
   "source": [
    "Notice that after the execution of `preprocessingOneStep` the line `!mkdir -p {path}` is runned because the `!` symbol let us work with the terminal prompt and create a directory to save the numpy files for each set.\n",
    "\n",
    "After creating the folder we use `saveData` for save the files in your Google Drive directory. This is a fundamental step because the model notebook is going to take the data just reading the files; that way we do not have to repeat the preprocessing everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e13d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(en_train,en_test,en_val) = readConlluDataset(base_url,file_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e72f7a",
   "metadata": {},
   "source": [
    "### Obtaining UPOS from each language\n",
    "\n",
    "The goal of this work is to predict UPOS from text, UPOS could change when you change the language so we need to identify first how many UPOS (and it names) we have and create a dictionary with them.\n",
    "\n",
    "`getUposList` is a function that expect a the train-text and extract unique UPOS from it. We ignore the \"\\_\" UPOS because we are not going to use them in our data. After extracting different UPOS we create two dictionaris that contains the transformation from UPO to number and the oposite. With an output like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/5.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUposList(sentences):\n",
    "  total_upos = list()\n",
    "  for sentence in sentences:\n",
    "    for detail in sentence:\n",
    "      current_upos = detail['upos']\n",
    "      if (current_upos != '_') and (current_upos not in total_upos):\n",
    "        total_upos.append(current_upos)\n",
    "  number2upo = {}\n",
    "  upo2number = {}\n",
    "  for i in range(0,len(total_upos)):\n",
    "    upo2number[total_upos[i]] = i+1\n",
    "    number2upo[i+1] = total_upos[i]\n",
    "  print(\"Total of different UPOS: \",len(total_upos))\n",
    "  print(upo2number)\n",
    "  return (upo2number,number2upo,len(total_upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837ab48",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "In this step we can to create our \"final\" data set wit the sentences and UPOS, each row has an array containing each sentence with word represented by numbers and an array with the equivalent UPOS for the previous words.\n",
    "\n",
    "- The transformation of UPOS will be made with the previous function using the dictionaries\n",
    "- The transformation of text will be made with the tokenizer\n",
    "\n",
    "#### Tokenizer\n",
    "\n",
    "We will take advantage to explain this preprocessing abording the creation of tokenizer.\n",
    "\n",
    "1. Now the variable `en_train` contain a conllu data structure, but it is different to a dataset, we can transform that conllu data into pandas Data frame by using the `conlluToDataset` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6106bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_df = conlluToDataset(en_train,en_upo2number) #1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conlluToDataset(sentences,upos2number):\n",
    "  from conllu import parse\n",
    "  df = pd.DataFrame({'sentences' : [],'UPOS' : []})\n",
    "  for i in range(0,len(sentences)):\n",
    "    sentence = pd.DataFrame.from_dict(sentences[i][:])['form'].values\n",
    "    upos = pd.DataFrame.from_dict(sentences[i][:])['upos'].values\n",
    "    results = np.where(upos == \"_\")\n",
    "    if len(results[0])>0:\n",
    "        sentence = np.delete(sentence, results)\n",
    "        upos = np.delete(upos, results)\n",
    "    numb_upos = [upos2number[upo] for upo in upos]\n",
    "    new_row = pd.Series({'sentences': sentence, 'UPOS': numb_upos})\n",
    "    df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de860f7a",
   "metadata": {},
   "source": [
    "This function read every sentence separated by words an the corresponding UPOS for each word. After that it is time tu put them in a Data Frame with two columns `sentences` and `UPOS`. Like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/6.png\" width=\"700\">\n",
    "\n",
    "There are some word and UPOS that we don't have to consider, because when we take form and UPOS; word like \"don't\" are represented with 3 words: \"don't\", 'do', \"n't\". In this case we are not considering \"don't\" and \"_\" (UPOS).\n",
    "\n",
    "2. To train the Tokenizer we are going to create a new column with this preprossing text, this new text will be the data that the Tokenizer will use to be trained.\n",
    "\n",
    "`addTextColumn` function create the column `sentence_as_string` that contains the same text on column `text` but as a paragraph.\n",
    "\n",
    "3. We will use this trainin data set's column to train the Tokenizer. We set the `filter` parameter without special symbols because we do not want to avoid that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b106e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_df = addTextColumn(en_train_df) #2 \n",
    "(en_tokenizer,en_word_index) = trainTokenizer(en_train_df[\"sentence_as_string\"]) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeda8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTextColumn(df):\n",
    "    df2 = df.copy()\n",
    "    for row in range(0,len(df2)):\n",
    "      empty_string = \"\"\n",
    "      for element in df2.loc[row,'sentences']:\n",
    "        empty_string = empty_string + \" \" + element\n",
    "      df2.loc[row,\"sentence_as_string\"] = empty_string\n",
    "    return df2\n",
    "\n",
    "def trainTokenizer(sentences,oov_token=\"<OOV>\",filters=\"\"):\n",
    "  from keras.preprocessing.text import Tokenizer\n",
    "  if(isinstance(sentences,list)):\n",
    "    text_list = list()\n",
    "    for sentence in sentences:\n",
    "      text = sentence.metadata['text']\n",
    "      text_list.append(text)\n",
    "  else:\n",
    "    text_list = sentences\n",
    "  tokenizer = Tokenizer(oov_token=oov_token,filters=filters) \n",
    "  tokenizer.fit_on_texts(text_list)\n",
    "  word_index = tokenizer.word_index\n",
    "  print(\"Tokenizer trained! With \",len(word_index),\" words\")\n",
    "  return (tokenizer,word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cd176",
   "metadata": {},
   "source": [
    "4. Finally we apply this trained tokenizer to the dataset in `sentences` columns. After converting words in number we pad the sequences and transform our target into categorical variable; it is important to notice that in pad process we set a maximum words variable `max_len` in this case is equal to `128`. Thus, for instance, we have the following:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3624cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "(x_train_en,y_train_en) = applyTokenizer(en_train_df,en_tokenizer,max_len) #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTokenizer(df,tokenizer,maxlen):\n",
    "    df['text_tokenized'] = pd.NA\n",
    "    for row in range(0,len(df)):\n",
    "      tokenized = tokenizer.texts_to_sequences([df.loc[row,\"sentence_as_string\"]])\n",
    "      df.loc[:,'text_tokenized'].loc[row] = tokenized[0]\n",
    "    df = df[['text_tokenized','UPOS']]\n",
    "    pad_y = tf.keras.utils.pad_sequences(df['UPOS'],maxlen=maxlen)\n",
    "    y_set = tf.keras.utils.to_categorical(pad_y, dtype='float32')\n",
    "    x_set = tf.keras.utils.pad_sequences(df['text_tokenized'],maxlen=maxlen)\n",
    "    return (x_set,y_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21473b",
   "metadata": {},
   "source": [
    "In summary, we did the following process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "\n",
    "#English\n",
    "en_train_df = conlluToDataset(en_train,en_upo2number) #1 \n",
    "en_train_df = addTextColumn(en_train_df) #2 \n",
    "(en_tokenizer,en_word_index) = trainTokenizer(en_train_df[\"sentence_as_string\"]) #3\n",
    "(x_train_en,y_train_en) = applyTokenizer(en_train_df,en_tokenizer,max_len) #4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f767d4e",
   "metadata": {},
   "source": [
    "### Preprocessing without training the tokenizer.\n",
    "\n",
    "To use the other sets, it needs to pass for the same processing **but** we should not train the Tokenizer everytime (just with the training set). That is why we split the Tokenizer processes in two functions as we see before (`trainTokenizer` and `applyTokenizer`), so now we can do the same with the test and validation sets avoiding the `trainTokenizer` step. To keep it simple we have a function that calls the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreprocessing(sentences,upos2number,tokenizer,maxlen=128):\n",
    "  df = conlluToDataset(sentences,upos2number)\n",
    "  df = addTextColumn(df)\n",
    "  (x_set,y_set) = applyTokenizer(df,tokenizer,maxlen)\n",
    "  return (x_set,y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f774d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_test_es,y_test_es) = textPreprocessing(es_test,es_upo2number,es_tokenizer,max_len)\n",
    "(x_val_es,y_val_es)  = textPreprocessing(es_val,es_upo2number,es_tokenizer,max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe7d3d",
   "metadata": {},
   "source": [
    "## Preprocessing for Char Based Models\n",
    "\n",
    "In the case of character-based models, the preprocessing is very similar, since it takes the words that we separated in the previous steps to be able to review each one and it will convert it into a vector of new numbers, but this time they represent letters.\n",
    "\n",
    "Below are the `trainCharTokenizer` and `applyCharTokenizer` functions that are in charge of training the tokenizer and applying it to the texts in order to obtain input from our second type of neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21743292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCharTokenizer(sentences):\n",
    "  from keras.preprocessing.text import Tokenizer\n",
    "  text_list = sentences\n",
    "  tokenizer = Tokenizer(sentences,filters=\"\",char_level=True,lower=False) \n",
    "  tokenizer.fit_on_texts(text_list)\n",
    "  char_word_index = tokenizer.word_index\n",
    "  print(\"Tokenizer trained! With \",len(char_word_index),\" chars\")\n",
    "  return (tokenizer,char_word_index)\n",
    "\n",
    "def applyCharTokenizer(df,char_word_index,max_len_words,max_len_chars):\n",
    "  df['text_tokenized_char'] = pd.NA\n",
    "  for index,sentence in enumerate(df['sentences']):\n",
    "    sentence_array = []\n",
    "    for word in sentence:\n",
    "      word_array = []\n",
    "      for char in word:\n",
    "        if char in char_word_index:\n",
    "          word_array.append(char_word_index[char])\n",
    "        else:\n",
    "          word_array.append(0)\n",
    "      sentence_array.append(word_array)\n",
    "      df.loc[:,'text_tokenized_char'].loc[index] = sentence_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f5d5b",
   "metadata": {},
   "source": [
    "In short, these new functions will receive the same sentences but this time being enabled to discriminate between letters. These new functions have been added to the preprocessing to be performed all together on the datasets in the three languages, such that the preprocessing would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26393c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "max_len_chars = 25\n",
    "\n",
    "#English\n",
    "en_train_df = conlluToDataset(en_train,en_upo2number)\n",
    "en_train_df = addTextColumn(en_train_df)\n",
    "(en_tokenizer,en_word_index) = trainTokenizer(en_train_df[\"sentence_as_string\"])\n",
    "(en_char_tokenizer,en_char_word_index) = trainCharTokenizer(en_train_df[\"sentence_as_string\"])\n",
    "(x_train_en,y_train_en) = applyTokenizer(en_train_df,en_tokenizer,max_len)\n",
    "x_char_train_en = applyCharTokenizer(en_train_df,en_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "(x_test_en,y_test_en,x_char_test_en) = textPreprocessing(en_test,en_upo2number,en_tokenizer,en_char_word_index,max_len,max_len_chars)\n",
    "(x_val_en,y_val_en,x_char_val_en) = textPreprocessing(en_val,en_upo2number,en_tokenizer,en_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "#Spanish\n",
    "es_train_df = conlluToDataset(es_train,es_upo2number)\n",
    "es_train_df = addTextColumn(es_train_df)\n",
    "(es_tokenizer,es_word_index) = trainTokenizer(es_train_df[\"sentence_as_string\"])\n",
    "(es_char_tokenizer,es_char_word_index) = trainCharTokenizer(es_train_df[\"sentence_as_string\"])\n",
    "(x_train_es,y_train_es) = applyTokenizer(es_train_df,es_tokenizer,max_len)\n",
    "x_char_train_es = applyCharTokenizer(es_train_df,es_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "(x_test_es,y_test_es,x_char_test_es) = textPreprocessing(es_test,es_upo2number,es_tokenizer,es_char_word_index,max_len,max_len_chars)\n",
    "(x_val_es,y_val_es,x_char_val_es) = textPreprocessing(es_val,es_upo2number,es_tokenizer,es_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "#French\n",
    "fr_train_df = conlluToDataset(fr_train,fr_upo2number)\n",
    "fr_train_df = addTextColumn(fr_train_df)\n",
    "(fr_tokenizer,fr_word_index) = trainTokenizer(fr_train_df[\"sentence_as_string\"])\n",
    "(fr_char_tokenizer,fr_char_word_index) = trainCharTokenizer(fr_train_df[\"sentence_as_string\"])\n",
    "(x_train_fr,y_train_fr) = applyTokenizer(fr_train_df,fr_tokenizer,max_len)\n",
    "x_char_train_fr = applyCharTokenizer(fr_train_df,fr_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "(x_test_fr,y_test_fr,x_char_test_fr) = textPreprocessing(fr_test,fr_upo2number,fr_tokenizer,fr_char_word_index,max_len,max_len_chars)\n",
    "(x_val_fr,y_val_fr,x_char_val_fr) = textPreprocessing(fr_val,fr_upo2number,fr_tokenizer,fr_char_word_index,max_len,max_len_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088f7fd",
   "metadata": {},
   "source": [
    "This is the end of the Preprocessing Steps, you can repeat with others languages.\n",
    "\n",
    "## Building models\n",
    "\n",
    "### Word Based Models\n",
    "\n",
    "For the models we have two types of functions, in this first case it will be possible to execute the architectures that expect only words as input. The `builAnnModel` function is the one intended to receive this type of ANN such that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def builAnnModel(word_index,n_upos,maxlen_sentences=128,embedd_output=50,\n",
    "                 lstm_input=50,loss='categorical_crossentropy',\n",
    "                 optimizer='adam',metrics=['accuracy']):\n",
    "  tokenizer_size = len(word_index)+1\n",
    "  n_upos = n_upos + 1\n",
    "  inputs = tf.keras.Input(shape=(maxlen_sentences,),name=\"Input_Layer\")\n",
    "  embedd = tf.keras.layers.Embedding(tokenizer_size,embedd_output,mask_zero=True,name=\"Embedding_Layer\")(inputs)\n",
    "  lstm = tf.keras.layers.LSTM(lstm_input,return_sequences=True,name=\"LSTM_Layer\")(embedd)\n",
    "  outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_upos,activation='softmax'),name=\"TimeD_Layer\")(lstm)\n",
    "  model = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "  model.compile(loss = loss,optimizer = optimizer,metrics=[metrics])\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47d641",
   "metadata": {},
   "source": [
    "You can run this lines like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b764a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = builAnnModel(en_word_index,en_nupos,maxlen_sentences=128,embedd_output=50,lstm_input=50)\n",
    "en_model.fit(x_train_en, y_train_en, epochs=20, verbose=1, batch_size=200,validation_data=(x_val_en,y_val_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847d46a",
   "metadata": {},
   "source": [
    "### Word Based Models\n",
    "\n",
    "For the models we have two types of functions, the second case it will be possible to execute the architectures that expect  words and chars as input. The `builCharAnnModel` function is the one intended to receive this type of ANN such that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def builCharAnnModel(word_index,char_word_index,n_upos,maxlen_sentences=128,maxlen_chars=25,embedd_output=50,\n",
    "                 lstm_input=50,loss='categorical_crossentropy',\n",
    "                 optimizer='adam',metrics=['accuracy']):\n",
    "\n",
    "    tokenizer_size = len(word_index)+1\n",
    "    char_tokenier_size = len(char_word_index)+1\n",
    "    n_upos = n_upos + 1\n",
    "\n",
    "    inputs_words = tf.keras.Input(shape=(maxlen_sentences,),name=\"Input_Words_Layer\") # shape (rows,128)\n",
    "    inputs_chars = tf.keras.Input(shape=(maxlen_sentences,maxlen_chars,),name=\"Input_Chars_Layer\") #shape (rows,128,25)\n",
    "    embedd_words = tf.keras.layers.Embedding(tokenizer_size,embedd_output,mask_zero=True,name=\"Embedding_Word_Layer\")(inputs_words)\n",
    "    embedd_chars = tf.keras.layers.Embedding(char_tokenier_size,embedd_output,mask_zero=True,name=\"Embedding_char_Layer\")(inputs_chars)\n",
    "    lstm_chars = tf.keras.layers.TimeDistributed(tf.keras.layers.LSTM(embedd_output,return_sequences=False,name=\"LSTM_char_Layer\"))(embedd_chars) #distributing to each char\n",
    "    concatted = tf.keras.layers.Concatenate()([embedd_words, lstm_chars])\n",
    "    lstm = tf.keras.layers.LSTM(lstm_input,return_sequences=True,name=\"LSTM_Layer\")(concatted)\n",
    "    outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_upos,activation='softmax'),name=\"TimeD_Layer\")(lstm)\n",
    "    model = tf.keras.Model(inputs=[inputs_words,inputs_chars],outputs=outputs)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ca0d7",
   "metadata": {},
   "source": [
    "You can run this lines like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050708d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "fr_char_model = builCharAnnModel(fr_word_index,fr_char_word_index,fr_nupos,maxlen_sentences=128,maxlen_chars=25,embedd_output=50,lstm_input=batch_size)\n",
    "fr_char_model.fit([x_train_fr,x_char_train_fr], y_train_fr, epochs=12, verbose=1, batch_size=batch_size,validation_data=([x_val_fr,x_char_val_fr],y_val_fr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

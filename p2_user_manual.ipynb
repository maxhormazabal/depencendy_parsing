{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66117ff6",
   "metadata": {},
   "source": [
    "# P2 - User manual\n",
    "\n",
    "**Students:** Maximiliano Hormaz√°bal - Mutaz Abueisheh\n",
    "\n",
    "This project is available on GitHub. [Click here to go to the repository](https://github.com/maxhormazabal/depencendy_parsing)\n",
    "\n",
    "### 0. Identify the files:\n",
    "\n",
    "This project have two main files:\n",
    "- `p2_preprocessing.ipynb`: Is the one you have to use to know all the steps for get the raw data, make the transformations and load the final data set to use it after.\n",
    "- `p2_ann_models.ipynb`: Here you have all the model's arquitectures, in this program we do not transform the data it is just for experiments and get results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7f1c33",
   "metadata": {},
   "source": [
    "### 1. Install/Import libraries\n",
    "\n",
    "In this project files and folders are going to be created in some steps. If you are going to work on Google Colab it is crucial to connect your notebook to your Google Drive profile for files/folders managment doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting access to Google Drive files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7f1c33",
   "metadata": {},
   "source": [
    "\n",
    "The first step is to import the libraries we are going to use (or install if is necessary), the most important are:\n",
    "- Conllu to read the languages files correctly (in this case we will use parse module)\n",
    "- Tensorflow to work with ANN, Tokenizer and other Machine Learning Tools\n",
    "- Pandas to create/transform dataframes\n",
    "- Numpy to work with numbers and some data structures\n",
    "\n",
    "To install you can use `!pip install <name>` to install the specific tool that you need.\n",
    "\n",
    "As following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Setting working directory and importing functions\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/MASTER\") # <- Folder where you saved the utils\n",
    "from nlu_model_utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85f7154a",
   "metadata": {},
   "source": [
    "## Preprocessing Notebook\n",
    "\n",
    "### 2. Download the langagues datasets\n",
    "\n",
    "To find the dataset that you want to use visit this <a href=\"https://github.com/UniversalDependencies/\" target=\"_blank\">Github Repository</a>, there are a many posibilities of languague. If you are looking for an specific language the \"Repositories\" search bar is useful to it. \n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/1.png\" width=\"700\">\n",
    "\n",
    "When you are ready do the following:\n",
    "- Go to the repository of your selected language\n",
    "- As you see you have 3 files: training, testing, validation (train, test, dev)\n",
    "\n",
    "For instance\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/2.png\" width=\"700\">\n",
    "\n",
    "- Click on one of them (could be anyone) and then click on \"View raw\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/3.png\" width=\"700\">\n",
    "\n",
    "- Here there is the raw file, this is the information that we need, our data. Just copy the URL.\n",
    "\n",
    "<div style=\"border: 1px solid #2596be;padding:10px;\">\n",
    "    <p style=\"text-align: center;\">**Important:** You do not need to repeat this process for thi others two files. We need only one URL for **each language**.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/4.png\" width=\"700\">\n",
    "\n",
    "### 3. Read the data with Python\n",
    "\n",
    "Now we have the address of the data, let's read this file and save as a Python data structure. This is an example with _English Data_ using the URL `https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu` ins this case is the URL for training but as we wrote previously, does not matter wich data set you chose. We just need one of them.\n",
    "\n",
    "Distribute the URL in two variables:\n",
    "\n",
    "    - base_url: Contain the URL until \"/master/\"\n",
    "    - file_basename: Contain the name of the language without the last score\n",
    "    \n",
    "It will be clear with the next example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680500a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "# 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu'\n",
    "\n",
    "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/'\n",
    "file_basename = 'en_ewt-ud'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2b0e3d9",
   "metadata": {},
   "source": [
    "After you did this with all the languages that you want we can start with the preprocessing.\n",
    "\n",
    "<div style=\"border: 1px solid #2596be;padding:10px;\">\n",
    "    <p style=\"text-align: center;\"> <strong> Important:</strong> We are going to use self-made functions to make this process more understandable, the finall cell contains all the functions to be runned together (We must have this functions in memory to make the notebook works.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7533999",
   "metadata": {},
   "source": [
    "## 4. Preprocessing data\n",
    "\n",
    "The function `preprocessingOneStep` contains all the preprocessing steps just in one function. The output are the data sets and the path of that data. Actually the notebook of preproceesing is only in charge of this part of the project and the data we will use in models will be saved as files and the mentioned relative path.\n",
    "\n",
    "With the variables:\n",
    "\n",
    "```\n",
    "stack_len = 7\n",
    "buffer_len = 10\n",
    "```\n",
    "\n",
    "You can set the size of stack and buffer for the sets. Run this cell everytime you need to create a new data source for models\n",
    "\n",
    "The code is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-ParTUT/master/'\n",
    "file_basename = 'en_partut-ud'\n",
    "stack_len = 7\n",
    "buffer_len = 10\n",
    "\n",
    "(path,\n",
    "x_train_token,action_encod_train,deprel_encod_train,\n",
    "x_test_token,action_encod_test,deprel_encod_test,\n",
    "x_val_token,action_encod_val,deprel_encod_val) = preprocessingOneStep(base_url,file_basename,stack_len,buffer_len)\n",
    "\n",
    "\n",
    "!mkdir -p {path}\n",
    "\n",
    "\n",
    "saveData(path,x_train_token,action_encod_train,deprel_encod_train,x_test_token,action_encod_test,deprel_encod_test,x_val_token,action_encod_val,deprel_encod_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "014acd0e",
   "metadata": {},
   "source": [
    "Notice that after the execution of `preprocessingOneStep` the line `!mkdir -p {path}` is runned because the `!` symbol let us work with the terminal prompt and create a directory to save the numpy files for each set.\n",
    "\n",
    "After creating the folder we use `saveData` for save the files in your Google Drive directory. This is a fundamental step because the model notebook is going to take the data just reading the files; that way we do not have to repeat the preprocessing everytime.\n",
    "\n",
    "**If you run many times the folder should be like this**\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal/depencendy_parsing/main/img/nlu_data_folder.png\" width=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e72f7a",
   "metadata": {},
   "source": [
    "## Models Notebook\n",
    "\n",
    "After generating the data needed to train the models, the neural network tests begin. For this it is necessary to know the relevant functions:\n",
    "\n",
    "1. `buildModelA` allows us to build and compile the model selecting all the important parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelA(stack_len,buffer_len,action_shape,deprel_shape,optimizer='adam',learning_rate=0.01,embedd_output=50,loss='categorical_crossentropy'):\n",
    "  input1 = tf.keras.layers.Input(shape=(stack_len,),name = 'Stack_Input')\n",
    "  input2 = tf.keras.layers.Input(shape=(buffer_len,),name = 'Buffer_Input')\n",
    "\n",
    "  embedding_layer = tf.keras.layers.Embedding(10000, embedd_output,mask_zero=True)\n",
    "  input1_embedded = embedding_layer(input1)\n",
    "  input2_embedded = embedding_layer(input2)\n",
    "\n",
    "  lstm1 = tf.keras.layers.LSTM(embedd_output,return_sequences=False,name=\"LSTM_Layer1\")(input1_embedded)\n",
    "  lstm2 = tf.keras.layers.LSTM(embedd_output,return_sequences=False,name=\"LSTM_Layer2\")(input2_embedded)\n",
    "\n",
    "  # Concatenamos a lo largo del √∫ltimo eje\n",
    "  merged = tf.keras.layers.Concatenate(axis=1,name = 'Concat_Layer')([lstm1, lstm2])\n",
    "  dense1 = tf.keras.layers.Dense(50, activation='sigmoid', use_bias=True,name = 'Dense_Layer1')(merged)\n",
    "  dense2 = tf.keras.layers.Dense(15, input_dim=1, activation='relu', use_bias=True,name = 'Dense_Layer2')(dense1)\n",
    "  dense3 = tf.keras.layers.Dense(30, input_dim=1, activation='relu', use_bias=True,name = 'Dense_Layer3')(dense2)\n",
    "  output1 = tf.keras.layers.Dense(action_shape, activation='softmax', use_bias=True,name = 'Action_Output')(dense3)\n",
    "  output2 = tf.keras.layers.Dense(deprel_shape, activation='softmax', use_bias=True,name = 'Deprel_Output')(dense3)\n",
    "\n",
    "  model = tf.keras.Model(inputs=[input1,input2],outputs=[output1,output2])\n",
    "  \n",
    "  if(optimizer.lower() == 'adam'):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  elif(optimizer.lower() == 'sgd'):\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "  elif(optimizer.lower() == 'rmsprop'):\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adamw'):\n",
    "    opt = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adadelta'):\n",
    "    opt = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adagrad'):\n",
    "    opt = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adamax'):\n",
    "    opt = tf.keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'adafactor'):\n",
    "    opt = tf.keras.optimizers.Adafactor(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'nadam'):\n",
    "    opt = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "  elif (optimizer.lower() == 'ftrl'):\n",
    "    opt = tf.keras.optimizers.Ftrl(learning_rate=learning_rate)\n",
    "  else:\n",
    "    print('Optimizer not properly defined')\n",
    "\n",
    "  model.compile(loss=loss,optimizer=opt,metrics=['accuracy'])\n",
    "  return(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fba503c1",
   "metadata": {},
   "source": [
    "2. `fitModel` use the model created with the previous function and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "              x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "              x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "              model,\n",
    "              stopper,patience=3,epochs=10,batch_size=128):\n",
    "  callback = tf.keras.callbacks.EarlyStopping(monitor=stopper, patience=patience,restore_best_weights=True)\n",
    "  model.fit([x_train_stack,x_train_buffer],\n",
    "            [action_train, deprel_train],\n",
    "            epochs=epochs, batch_size=batch_size,\n",
    "            callbacks=[callback],\n",
    "            verbose = 0,\n",
    "            validation_data=([x_val_stack,x_val_buffer],[action_val,deprel_val]))\n",
    "  score = model.evaluate([x_test_stack,x_test_buffer],[action_test, deprel_test], verbose=0)\n",
    "  return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6837ab48",
   "metadata": {},
   "source": [
    "3. Finally `saveModelAData` is a function that can be used to test different configuration of an specific parameter. It trains a model changing the selected pivot parameter and save the results as a dataframe. The output of this function is the summarized data set and the best model of each execution.\n",
    "\n",
    "**Important**: The metrics are calcutated with the evaluation function (Keras) using the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6106bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModelAData(pivot_name,pivot,\n",
    "                   x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "                   x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "                   x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "                   stack_len,buffer_len,\n",
    "                   stopper,patience,\n",
    "                   batch_size,epochs,\n",
    "                   optimizer,learning_rate,\n",
    "                   embedd_output=50):\n",
    "  # Creating empty lists\n",
    "  arquitecture_set = []\n",
    "  stack_set = []\n",
    "  buffer_set = []\n",
    "  action_accuracy_set = []\n",
    "  deprel_accuracy_set = []\n",
    "  action_loss_set = []\n",
    "  deprel_loss_set = []\n",
    "  batch_size_set = []\n",
    "  epochs_set = []\n",
    "  optimizer_set = []\n",
    "  learning_rate_set = []\n",
    "  embedd_output_set = []\n",
    "  early_stop_set = []\n",
    "  time_set = []\n",
    "\n",
    "  models = []\n",
    "\n",
    "\n",
    "  for (index,value) in enumerate(pivot):\n",
    "\n",
    "    print('Starting execution where ',pivot_name,' varies, now with the value(s) ',value,'.')\n",
    "\n",
    "    if (pivot_name == 'batch_size'):\n",
    "      batch_size = value\n",
    "    elif (pivot_name == 'epochs'):\n",
    "      epochs = value\n",
    "    elif (pivot_name == 'early_stop'):\n",
    "      (stopper,patience) = value\n",
    "    elif (pivot_name == 'optimizer'):\n",
    "      (optimizer,learning_rate) = value\n",
    "\n",
    "    arquitecture = 'A'\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = buildModelA(stack_len,buffer_len,action_train[0].shape[0],deprel_train[0].shape[0],optimizer=optimizer,learning_rate=learning_rate,embedd_output=embedd_output,loss='categorical_crossentropy')\n",
    "\n",
    "    score = fitModel(x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "                  x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "                  x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "                  model,\n",
    "                  stopper,patience,epochs,batch_size)\n",
    "    loss,Action_Output_loss,Deprel_Output_loss,Action_Output_accuracy,Deprel_Output_accuracy = score\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "    # Saving models\n",
    "    models.append(model)\n",
    "\n",
    "    # Append values\n",
    "    arquitecture_set.append(arquitecture)\n",
    "    stack_set.append(stack_len)\n",
    "    buffer_set.append(buffer_len)\n",
    "    action_accuracy_set.append(Action_Output_accuracy)\n",
    "    deprel_accuracy_set.append(Deprel_Output_accuracy)\n",
    "    action_loss_set.append(Action_Output_loss)\n",
    "    deprel_loss_set.append(Deprel_Output_loss)\n",
    "    batch_size_set.append(batch_size)\n",
    "    epochs_set.append(epochs)\n",
    "    optimizer_set.append(optimizer)\n",
    "    learning_rate_set.append(learning_rate)\n",
    "    embedd_output_set.append(embedd_output)\n",
    "    early_stop_set.append(stopper)\n",
    "    time_set.append(training_time)\n",
    "\n",
    "  # Data dictionary\n",
    "\n",
    "  resultDict = {\n",
    "    'arquitecture' : arquitecture_set,\n",
    "    'stack' : stack_set,\n",
    "    'buffer' : buffer_set,\n",
    "    'action_accuracy' : action_accuracy_set,\n",
    "    'deprel_accuracy' : deprel_accuracy_set,\n",
    "    'action_loss' : action_loss_set,\n",
    "    'deprel_loss' : deprel_loss_set,\n",
    "    'batch_size' : batch_size_set,\n",
    "    'epochs' : epochs_set,\n",
    "    'optimizer' : optimizer_set,\n",
    "    'learning_rate' : learning_rate_set,\n",
    "    'embedd_output' : embedd_output,\n",
    "    'early_stop_set' : early_stop_set,\n",
    "    'time' : time_set,\n",
    "  }\n",
    "\n",
    "  return (pd.DataFrame(resultDict),models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "962d2f8f",
   "metadata": {},
   "source": [
    "One execution of those three function together is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ef8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different optimizers\n",
    "\n",
    "pivot_name = 'optimizer'\n",
    "pivot = [\n",
    "    ('adam' , 0.001),\n",
    "    ('adam' , 0.01),\n",
    "    ('adam' , 0.1),\n",
    "    ('sgd' , 0.001),\n",
    "    ('sgd' , 0.01),\n",
    "    ('sgd' , 0.1),\n",
    "    ('rmsprop' , 0.001),\n",
    "    ('rmsprop' , 0.01),\n",
    "    ('rmsprop' , 0.1),\n",
    "    ('adadelta' , 0.001),\n",
    "    ('adadelta' , 0.01),\n",
    "    ('adadelta' , 0.1),\n",
    "    ('adagrad' , 0.001),\n",
    "    ('adagrad' , 0.01),\n",
    "    ('adagrad' , 0.1),\n",
    "    ('adamax' , 0.001),\n",
    "    ('adamax' , 0.01),\n",
    "    ('adamax' , 0.1),\n",
    "    ('nadam' , 0.001),\n",
    "    ('nadam' , 0.01),\n",
    "    ('nadam' , 0.1),\n",
    "    ('ftrl' , 0.001),\n",
    "    ('ftrl' , 0.01),\n",
    "    ('ftrl' , 0.1)\n",
    "]\n",
    "\n",
    "# Setting values\n",
    "stack_len = 3\n",
    "buffer_len = 3\n",
    "arquitecture = 'A'\n",
    "stopper = 'val_Deprel_Output_loss'\n",
    "patience = 3\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "optimizer = 'adam'\n",
    "learning_rate = 0.01\n",
    "embedd_output = 50\n",
    "\n",
    "(df_res_optimizers,models_optimizers) = saveModelAData(pivot_name,pivot,\n",
    "                   x_train_stack,x_train_buffer,action_train, deprel_train,\n",
    "                   x_val_stack,x_val_buffer,action_val,deprel_val,\n",
    "                   x_test_stack,x_test_buffer,action_test,deprel_test,\n",
    "                   stack_len,buffer_len,\n",
    "                   stopper,patience,\n",
    "                   batch_size,epochs,\n",
    "                   optimizer,learning_rate,\n",
    "                   embedd_output=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de860f7a",
   "metadata": {},
   "source": [
    "Where the data frame of result is like this\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal/depencendy_parsing/main/img/df_results_example.png\" width=\"700\">\n",
    "\n",
    "And you can find all the data set with the results (based on a pivot parameter) in the folder `model_testing` in `.csv` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b106e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_df = addTextColumn(en_train_df) #2 \n",
    "(en_tokenizer,en_word_index) = trainTokenizer(en_train_df[\"sentence_as_string\"]) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeda8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTextColumn(df):\n",
    "    df2 = df.copy()\n",
    "    for row in range(0,len(df2)):\n",
    "      empty_string = \"\"\n",
    "      for element in df2.loc[row,'sentences']:\n",
    "        empty_string = empty_string + \" \" + element\n",
    "      df2.loc[row,\"sentence_as_string\"] = empty_string\n",
    "    return df2\n",
    "\n",
    "def trainTokenizer(sentences,oov_token=\"<OOV>\",filters=\"\"):\n",
    "  from keras.preprocessing.text import Tokenizer\n",
    "  if(isinstance(sentences,list)):\n",
    "    text_list = list()\n",
    "    for sentence in sentences:\n",
    "      text = sentence.metadata['text']\n",
    "      text_list.append(text)\n",
    "  else:\n",
    "    text_list = sentences\n",
    "  tokenizer = Tokenizer(oov_token=oov_token,filters=filters) \n",
    "  tokenizer.fit_on_texts(text_list)\n",
    "  word_index = tokenizer.word_index\n",
    "  print(\"Tokenizer trained! With \",len(word_index),\" words\")\n",
    "  return (tokenizer,word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cd176",
   "metadata": {},
   "source": [
    "4. Finally we apply this trained tokenizer to the dataset in `sentences` columns. After converting words in number we pad the sequences and transform our target into categorical variable; it is important to notice that in pad process we set a maximum words variable `max_len` in this case is equal to `128`. Thus, for instance, we have the following:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/maxhormazabal-test/nlu-p1/main/7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3624cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "(x_train_en,y_train_en) = applyTokenizer(en_train_df,en_tokenizer,max_len) #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTokenizer(df,tokenizer,maxlen):\n",
    "    df['text_tokenized'] = pd.NA\n",
    "    for row in range(0,len(df)):\n",
    "      tokenized = tokenizer.texts_to_sequences([df.loc[row,\"sentence_as_string\"]])\n",
    "      df.loc[:,'text_tokenized'].loc[row] = tokenized[0]\n",
    "    df = df[['text_tokenized','UPOS']]\n",
    "    pad_y = tf.keras.utils.pad_sequences(df['UPOS'],maxlen=maxlen)\n",
    "    y_set = tf.keras.utils.to_categorical(pad_y, dtype='float32')\n",
    "    x_set = tf.keras.utils.pad_sequences(df['text_tokenized'],maxlen=maxlen)\n",
    "    return (x_set,y_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21473b",
   "metadata": {},
   "source": [
    "In summary, we did the following process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "\n",
    "#English\n",
    "en_train_df = conlluToDataset(en_train,en_upo2number) #1 \n",
    "en_train_df = addTextColumn(en_train_df) #2 \n",
    "(en_tokenizer,en_word_index) = trainTokenizer(en_train_df[\"sentence_as_string\"]) #3\n",
    "(x_train_en,y_train_en) = applyTokenizer(en_train_df,en_tokenizer,max_len) #4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f767d4e",
   "metadata": {},
   "source": [
    "### Preprocessing without training the tokenizer.\n",
    "\n",
    "To use the other sets, it needs to pass for the same processing **but** we should not train the Tokenizer everytime (just with the training set). That is why we split the Tokenizer processes in two functions as we see before (`trainTokenizer` and `applyTokenizer`), so now we can do the same with the test and validation sets avoiding the `trainTokenizer` step. To keep it simple we have a function that calls the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreprocessing(sentences,upos2number,tokenizer,maxlen=128):\n",
    "  df = conlluToDataset(sentences,upos2number)\n",
    "  df = addTextColumn(df)\n",
    "  (x_set,y_set) = applyTokenizer(df,tokenizer,maxlen)\n",
    "  return (x_set,y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f774d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_test_es,y_test_es) = textPreprocessing(es_test,es_upo2number,es_tokenizer,max_len)\n",
    "(x_val_es,y_val_es)  = textPreprocessing(es_val,es_upo2number,es_tokenizer,max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe7d3d",
   "metadata": {},
   "source": [
    "## Preprocessing for Char Based Models\n",
    "\n",
    "In the case of character-based models, the preprocessing is very similar, since it takes the words that we separated in the previous steps to be able to review each one and it will convert it into a vector of new numbers, but this time they represent letters.\n",
    "\n",
    "Below are the `trainCharTokenizer` and `applyCharTokenizer` functions that are in charge of training the tokenizer and applying it to the texts in order to obtain input from our second type of neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21743292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCharTokenizer(sentences):\n",
    "  from keras.preprocessing.text import Tokenizer\n",
    "  text_list = sentences\n",
    "  tokenizer = Tokenizer(sentences,filters=\"\",char_level=True,lower=False) \n",
    "  tokenizer.fit_on_texts(text_list)\n",
    "  char_word_index = tokenizer.word_index\n",
    "  print(\"Tokenizer trained! With \",len(char_word_index),\" chars\")\n",
    "  return (tokenizer,char_word_index)\n",
    "\n",
    "def applyCharTokenizer(df,char_word_index,max_len_words,max_len_chars):\n",
    "  df['text_tokenized_char'] = pd.NA\n",
    "  for index,sentence in enumerate(df['sentences']):\n",
    "    sentence_array = []\n",
    "    for word in sentence:\n",
    "      word_array = []\n",
    "      for char in word:\n",
    "        if char in char_word_index:\n",
    "          word_array.append(char_word_index[char])\n",
    "        else:\n",
    "          word_array.append(0)\n",
    "      sentence_array.append(word_array)\n",
    "      df.loc[:,'text_tokenized_char'].loc[index] = sentence_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f5d5b",
   "metadata": {},
   "source": [
    "In short, these new functions will receive the same sentences but this time being enabled to discriminate between letters. These new functions have been added to the preprocessing to be performed all together on the datasets in the three languages, such that the preprocessing would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26393c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "max_len_chars = 25\n",
    "\n",
    "#English\n",
    "en_train_df = conlluToDataset(en_train,en_upo2number)\n",
    "en_train_df = addTextColumn(en_train_df)\n",
    "(en_tokenizer,en_word_index) = trainTokenizer(en_train_df[\"sentence_as_string\"])\n",
    "(en_char_tokenizer,en_char_word_index) = trainCharTokenizer(en_train_df[\"sentence_as_string\"])\n",
    "(x_train_en,y_train_en) = applyTokenizer(en_train_df,en_tokenizer,max_len)\n",
    "x_char_train_en = applyCharTokenizer(en_train_df,en_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "(x_test_en,y_test_en,x_char_test_en) = textPreprocessing(en_test,en_upo2number,en_tokenizer,en_char_word_index,max_len,max_len_chars)\n",
    "(x_val_en,y_val_en,x_char_val_en) = textPreprocessing(en_val,en_upo2number,en_tokenizer,en_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "#Spanish\n",
    "es_train_df = conlluToDataset(es_train,es_upo2number)\n",
    "es_train_df = addTextColumn(es_train_df)\n",
    "(es_tokenizer,es_word_index) = trainTokenizer(es_train_df[\"sentence_as_string\"])\n",
    "(es_char_tokenizer,es_char_word_index) = trainCharTokenizer(es_train_df[\"sentence_as_string\"])\n",
    "(x_train_es,y_train_es) = applyTokenizer(es_train_df,es_tokenizer,max_len)\n",
    "x_char_train_es = applyCharTokenizer(es_train_df,es_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "(x_test_es,y_test_es,x_char_test_es) = textPreprocessing(es_test,es_upo2number,es_tokenizer,es_char_word_index,max_len,max_len_chars)\n",
    "(x_val_es,y_val_es,x_char_val_es) = textPreprocessing(es_val,es_upo2number,es_tokenizer,es_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "#French\n",
    "fr_train_df = conlluToDataset(fr_train,fr_upo2number)\n",
    "fr_train_df = addTextColumn(fr_train_df)\n",
    "(fr_tokenizer,fr_word_index) = trainTokenizer(fr_train_df[\"sentence_as_string\"])\n",
    "(fr_char_tokenizer,fr_char_word_index) = trainCharTokenizer(fr_train_df[\"sentence_as_string\"])\n",
    "(x_train_fr,y_train_fr) = applyTokenizer(fr_train_df,fr_tokenizer,max_len)\n",
    "x_char_train_fr = applyCharTokenizer(fr_train_df,fr_char_word_index,max_len,max_len_chars)\n",
    "\n",
    "(x_test_fr,y_test_fr,x_char_test_fr) = textPreprocessing(fr_test,fr_upo2number,fr_tokenizer,fr_char_word_index,max_len,max_len_chars)\n",
    "(x_val_fr,y_val_fr,x_char_val_fr) = textPreprocessing(fr_val,fr_upo2number,fr_tokenizer,fr_char_word_index,max_len,max_len_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088f7fd",
   "metadata": {},
   "source": [
    "This is the end of the Preprocessing Steps, you can repeat with others languages.\n",
    "\n",
    "## Building models\n",
    "\n",
    "### Word Based Models\n",
    "\n",
    "For the models we have two types of functions, in this first case it will be possible to execute the architectures that expect only words as input. The `builAnnModel` function is the one intended to receive this type of ANN such that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def builAnnModel(word_index,n_upos,maxlen_sentences=128,embedd_output=50,\n",
    "                 lstm_input=50,loss='categorical_crossentropy',\n",
    "                 optimizer='adam',metrics=['accuracy']):\n",
    "  tokenizer_size = len(word_index)+1\n",
    "  n_upos = n_upos + 1\n",
    "  inputs = tf.keras.Input(shape=(maxlen_sentences,),name=\"Input_Layer\")\n",
    "  embedd = tf.keras.layers.Embedding(tokenizer_size,embedd_output,mask_zero=True,name=\"Embedding_Layer\")(inputs)\n",
    "  lstm = tf.keras.layers.LSTM(lstm_input,return_sequences=True,name=\"LSTM_Layer\")(embedd)\n",
    "  outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_upos,activation='softmax'),name=\"TimeD_Layer\")(lstm)\n",
    "  model = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "  model.compile(loss = loss,optimizer = optimizer,metrics=[metrics])\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47d641",
   "metadata": {},
   "source": [
    "You can run this lines like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b764a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = builAnnModel(en_word_index,en_nupos,maxlen_sentences=128,embedd_output=50,lstm_input=50)\n",
    "en_model.fit(x_train_en, y_train_en, epochs=20, verbose=1, batch_size=200,validation_data=(x_val_en,y_val_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847d46a",
   "metadata": {},
   "source": [
    "### Word Based Models\n",
    "\n",
    "For the models we have two types of functions, the second case it will be possible to execute the architectures that expect  words and chars as input. The `builCharAnnModel` function is the one intended to receive this type of ANN such that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def builCharAnnModel(word_index,char_word_index,n_upos,maxlen_sentences=128,maxlen_chars=25,embedd_output=50,\n",
    "                 lstm_input=50,loss='categorical_crossentropy',\n",
    "                 optimizer='adam',metrics=['accuracy']):\n",
    "\n",
    "    tokenizer_size = len(word_index)+1\n",
    "    char_tokenier_size = len(char_word_index)+1\n",
    "    n_upos = n_upos + 1\n",
    "\n",
    "    inputs_words = tf.keras.Input(shape=(maxlen_sentences,),name=\"Input_Words_Layer\") # shape (rows,128)\n",
    "    inputs_chars = tf.keras.Input(shape=(maxlen_sentences,maxlen_chars,),name=\"Input_Chars_Layer\") #shape (rows,128,25)\n",
    "    embedd_words = tf.keras.layers.Embedding(tokenizer_size,embedd_output,mask_zero=True,name=\"Embedding_Word_Layer\")(inputs_words)\n",
    "    embedd_chars = tf.keras.layers.Embedding(char_tokenier_size,embedd_output,mask_zero=True,name=\"Embedding_char_Layer\")(inputs_chars)\n",
    "    lstm_chars = tf.keras.layers.TimeDistributed(tf.keras.layers.LSTM(embedd_output,return_sequences=False,name=\"LSTM_char_Layer\"))(embedd_chars) #distributing to each char\n",
    "    concatted = tf.keras.layers.Concatenate()([embedd_words, lstm_chars])\n",
    "    lstm = tf.keras.layers.LSTM(lstm_input,return_sequences=True,name=\"LSTM_Layer\")(concatted)\n",
    "    outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_upos,activation='softmax'),name=\"TimeD_Layer\")(lstm)\n",
    "    model = tf.keras.Model(inputs=[inputs_words,inputs_chars],outputs=outputs)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ca0d7",
   "metadata": {},
   "source": [
    "You can run this lines like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050708d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "fr_char_model = builCharAnnModel(fr_word_index,fr_char_word_index,fr_nupos,maxlen_sentences=128,maxlen_chars=25,embedd_output=50,lstm_input=batch_size)\n",
    "fr_char_model.fit([x_train_fr,x_char_train_fr], y_train_fr, epochs=12, verbose=1, batch_size=batch_size,validation_data=([x_val_fr,x_char_val_fr],y_val_fr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
